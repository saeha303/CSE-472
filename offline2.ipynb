{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve,accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mainly reading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_6176\\1151504074.py:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataframe[column].fillna(mode_value,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.41100025638687926\n",
      "Sensitivity/Recall:  0.9544235924932976\n",
      "Specificity:  0.4063706563706564\n",
      "Precision:  0.3666323377960865\n",
      "F1 Score:  0.5297619047619048\n",
      "(1409, 13619)\n",
      "(13619, 1)\n",
      "[[ 1.         -0.43988526 -1.27735389 ... -0.01191574 -0.01191574\n",
      "  -0.01191574]\n",
      " [ 1.         -0.43988526  0.35134502 ... -0.01191574 -0.01191574\n",
      "  -0.01191574]\n",
      " [ 1.         -0.43988526  0.79923722 ... -0.01191574 -0.01191574\n",
      "  -0.01191574]\n",
      " ...\n",
      " [ 1.         -0.43988526 -0.62587433 ... -0.01191574 -0.01191574\n",
      "  -0.01191574]\n",
      " [ 1.         -0.43988526  1.49143426 ... -0.01191574 -0.01191574\n",
      "  -0.01191574]\n",
      " [ 1.         -0.43988526 -1.27735389 ... -0.01191574 -0.01191574\n",
      "  -0.01191574]]\n",
      "[[-15.14531688]\n",
      " [  0.71263846]\n",
      " [ -4.17499125]\n",
      " ...\n",
      " [  0.18046772]\n",
      " [ -0.27959666]\n",
      " [ -0.38726435]]\n",
      "0\n",
      "Churn    0\n",
      "dtype: int64\n",
      "AUROC:  0.8268940138913328\n",
      "AUPR:  0.6872713528915502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "dataframe=None\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.array(z, dtype=float)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logisticRegression(X, y, theta, learning_rate, maxIteration, noFeatures):\n",
    "    # print('hi')\n",
    "    m=len(y)\n",
    "    for iteration in range(maxIteration):\n",
    "        # matrix multiplication\n",
    "        h = sigmoid(np.dot(X,theta))\n",
    "        gradient = np.dot(X.T,(h - y))\n",
    "        theta = theta - learning_rate * gradient\n",
    "    return theta\n",
    "\n",
    "def predict(X, theta):\n",
    "    probabilities = sigmoid(X @ theta)\n",
    "    predictions = [1 if prob >= 0.5 else 0 for prob in probabilities]\n",
    "    return np.array(predictions)\n",
    "\n",
    "def scalingFunction(scaling='standard'):\n",
    "    if scaling == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaling == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "def normalize(X):\n",
    "    # m,n=X.shape\n",
    "    # for i in range(n):\n",
    "    #     X=(X-X.mean(axis=0))/X.std(axis=0)\n",
    "    return (X - X.mean()) / X.std()\n",
    "\n",
    "def preprocessing(target_col_name):\n",
    "    # drop null and fill null\n",
    "    dataframe.dropna(subset=[target_col_name], inplace=True)\n",
    "    dataframe.fillna(dataframe.mean(numeric_only=True),inplace=True)\n",
    "    # fill null for non-numeric columns\n",
    "    non_numerical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "    for column in non_numerical_columns:\n",
    "        mode_value = dataframe[column].mode()[0]\n",
    "        dataframe[column].fillna(mode_value,inplace=True)\n",
    "    # drop duplicates\n",
    "    dataframe.drop_duplicates(inplace=True)\n",
    "    # feature and target\n",
    "    features=dataframe.drop(target_col_name,axis=1)\n",
    "    target=dataframe[target_col_name]\n",
    "    # label encoding the target\n",
    "    encoder=LabelEncoder()\n",
    "    target=encoder.fit_transform(target)\n",
    "    # categorization and one-hot encoding\n",
    "    categorical_columns=features.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        features[col]=features[col].astype('category')\n",
    "    features=pd.get_dummies(features,columns=categorical_columns)\n",
    "    # scaling\n",
    "    candidate_columns=features.select_dtypes(exclude=['bool']).columns\n",
    "    scaler=scalingFunction('standard')\n",
    "    # scaler=scalingFunction('minmax')\n",
    "    features_scaled=features.copy()\n",
    "    features_scaled[candidate_columns]=scaler.fit_transform(features[candidate_columns])\n",
    "    # transform to dataframe\n",
    "    features_df=pd.DataFrame(features_scaled,columns=features.columns)\n",
    "    target_df=pd.DataFrame(target,columns=[target_col_name])\n",
    "    # adding for x0\n",
    "    features_df_normalized=normalize(features_df)\n",
    "    features_df_normalized.insert(0, 'x0', 1)\n",
    "    # split into datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_df_normalized, target_df, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    # to numpy array\n",
    "    X = X_train.to_numpy()\n",
    "    y = y_train.to_numpy()\n",
    "    maxIteration=1000\n",
    "    noFeatures=features_df_normalized.shape[1]\n",
    "    # I was stuck here....\n",
    "    theta = np.zeros((noFeatures,1))\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    theta_final= logisticRegression(X, y, theta, learning_rate, maxIteration, noFeatures)\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    predictions = predict(X_test_np, theta_final)\n",
    "    y_test_np = y_test.to_numpy()\n",
    "    accuracy = np.mean(predictions == y_test_np)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    sensitivity = recall_score(y_test, predictions)\n",
    "    print(\"Sensitivity/Recall: \", sensitivity)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    print(\"Precision: \", precision)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print(X_test_np.shape)\n",
    "    print(theta_final.shape)\n",
    "    print(X_test_np)\n",
    "    print(theta_final)\n",
    "    y_prob = sigmoid(np.dot(X_test_np,theta_final))  # Predicted probabilities\n",
    "    print(np.isnan(y_prob).sum())\n",
    "    print(np.isnan(y_test).sum())\n",
    "    auroc = roc_auc_score(y_test, y_prob)\n",
    "    print(\"AUROC: \", auroc)\n",
    "    precision_values, recall_values, _ = precision_recall_curve(y_test, y_prob)\n",
    "    aupr = auc(recall_values, precision_values)\n",
    "    print(\"AUPR: \", aupr)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test_np)\n",
    "    print(accuracy_score(y_test_np,y_pred))\n",
    "\n",
    "def read_2():\n",
    "    global dataframe\n",
    "    column_file = 'adult/adult.names'\n",
    "    data_file = 'adult/adult.data'\n",
    "    columns = []\n",
    "    with open(column_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if '|' not in line:  # Ignore lines starting with '|'\n",
    "                if ':' in line:\n",
    "                    # Extract the column name before the ':' punctuation mark\n",
    "                    col_name = line.split(':')[0].strip()\n",
    "                    columns.append(col_name)\n",
    "    columns.append('income-exceeds')\n",
    "    dataframe = pd.read_csv(data_file, header=None)\n",
    "    # print(dataframe)\n",
    "    dataframe.columns = columns\n",
    "    # print(dataframe)\n",
    "    dataframe.replace(' ?', np.nan, inplace=True)\n",
    "\n",
    "def read(input):\n",
    "    global dataframe\n",
    "    # file 1\n",
    "    if input == 1:\n",
    "        dataframe = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "        preprocessing('Churn')\n",
    "    # file 2\n",
    "    elif input == 2:\n",
    "        read_2()\n",
    "        preprocessing('income-exceeds')\n",
    "    # file 3\n",
    "    elif input == 3:\n",
    "        dataframe = pd.read_csv('creditcard.csv')\n",
    "        preprocessing('Class')\n",
    "    else:\n",
    "        print(\"Invalid input\")\n",
    "    # dataframe.head()\n",
    "\n",
    "user_input = int(input(\"Enter 1, 2 or 3: \"))\n",
    "read(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows,cols=dataframe.shape\n",
    "# cols,rows\n",
    "# dataframe.describe()\n",
    "# dataframe.isnull().sum()\n",
    "# dataframe.duplicated().sum()\n",
    "# Data cleaning\n",
    "# dataframe['Attrition'].isnull().sum()\n",
    "# # drop the rows where the target value is null\n",
    "# dataframe.dropna(subset=['Attrition'], inplace=True)\n",
    "# dataframe['Attrition'].isnull().sum()\n",
    "# dataframe.isnull().sum()\n",
    "# dataframe.shape\n",
    "# dataframe.fillna(dataframe.mean(numeric_only=True),inplace=True)\n",
    "# non_numerical_columns = dataframe.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "# for column in non_numerical_columns:\n",
    "#     mode_value = dataframe[column].mode()[0]\n",
    "#     dataframe[column].fillna(mode_value,inplace=True)\n",
    "# dataframe.isnull().sum()\n",
    "# dataframe.duplicated().sum()\n",
    "# dataframe.drop_duplicates(inplace=True)\n",
    "# dataframe.duplicated().sum()\n",
    "# # Creation of input and output features\n",
    "# features=dataframe.drop('Attrition',axis=1)\n",
    "# target=dataframe['Attrition']\n",
    "# #Conversion of features into numeric values\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# encoder=LabelEncoder()\n",
    "# target=encoder.fit_transform(target)\n",
    "# target\n",
    "# categorical_columns=features.select_dtypes(include=['object']).columns\n",
    "# for col in categorical_columns:\n",
    "#     features[col]=features[col].astype('category')\n",
    "# features=pd.get_dummies(features,columns=categorical_columns)\n",
    "# features\n",
    "# features.dtypes\n",
    "# Scaling of the features\n",
    "# candidate_columns=features.select_dtypes(exclude=['bool']).columns\n",
    "# candidate_columns\n",
    "# from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "# def scalingFunction(scaling='standard'):\n",
    "#     if scaling == 'standard':\n",
    "#         scaler = StandardScaler()\n",
    "#     elif scaling == 'minmax':\n",
    "#         scaler = MinMaxScaler()\n",
    "    \n",
    "#     return scaler\n",
    "\n",
    "# scaler=scalingFunction('standard')\n",
    "# # scaler=scalingFunction('minmax')\n",
    "# features_scaled=features.copy()\n",
    "# features_scaled[candidate_columns]=scaler.fit_transform(features[candidate_columns])\n",
    "# features_scaled\n",
    "\n",
    "#  Correlation Analysis\n",
    "# # transform from numpy array to dataframe\n",
    "# features_df=pd.DataFrame(features_scaled,columns=features.columns)\n",
    "# target_df=pd.DataFrame(target,columns=['Attrition'])\n",
    "# target_df\n",
    "# target_series=target_df['Attrition']\n",
    "# correlation=features_df.corrwith(target_series)\n",
    "# correlation\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "# features_with_target = pd.concat([features_df, target_series], axis=1)\n",
    "# correlation_matrix = features_with_target.corr()\n",
    "# plt.figure(figsize=(30, 30))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.1f')\n",
    "# plt.title('Correlation Matrix')\n",
    "# plt.show()\n",
    "# import numpy as np\n",
    "\n",
    "# features_df.reset_index(drop=True, inplace=True)\n",
    "# target_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# class_0=features_df.loc[target_df['Attrition']==0]\n",
    "# class_1=features_df.loc[target_df['Attrition']==1]\n",
    "\n",
    "# def plot(column):\n",
    "#     plt.plot(class_0[column],np.zeros_like(class_0[column]),'o',label='No')\n",
    "#     plt.plot(class_1[column],np.zeros_like(class_1[column]),'x',label='Yes')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.xlabel(column)\n",
    "#     plt.title(f\"1D scatter plot of {column}\")\n",
    "#     plt.show()\n",
    "\n",
    "# top_20_features = correlation.abs().nlargest(20).index\n",
    "# print(top_20_features)\n",
    "# for feature in top_20_features:\n",
    "#     plot(feature)\n",
    "\n",
    "# Validating the pipeline (Bonus Task)\n",
    "# top_20_features = correlation.index.tolist() #standard scaling\n",
    "# features_df = features_df[top_20_features]\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features_df, target_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
