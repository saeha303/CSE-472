{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve,accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=None\n",
    "dataframe2=None\n",
    "X_train = None\n",
    "X_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "X_val = None\n",
    "y_val = None\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self,maxIteration,noFeatures,learning_rate):\n",
    "        self.maxIteration=maxIteration\n",
    "        self.noFeatures=noFeatures\n",
    "        self.theta = np.zeros((noFeatures,1))\n",
    "        self.bias=0\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        z = np.array(z, dtype=float)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def logisticRegression(self,X,y):\n",
    "        # print('hi')\n",
    "        m=len(X)\n",
    "        for iteration in range(self.maxIteration):\n",
    "            # matrix multiplication\n",
    "            h = self.sigmoid(np.dot(X,self.theta)+self.bias)\n",
    "            gradient = np.dot(X.T,(h - y))/m\n",
    "            db=np.sum(h-y)/m\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self,X):\n",
    "        probabilities = self.sigmoid(np.dot(X,self.theta)+self.bias)\n",
    "        predictions = [1 if prob >= 0.5 else 0 for prob in probabilities]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def output(self,X,predictions):\n",
    "        y_test_np = y_test.flatten()\n",
    "\n",
    "        # predictions = self.predict(X)\n",
    "        # accuracy = np.mean(predictions == y_test_np)\n",
    "        # print(f\"Accuracy: {accuracy}\")\n",
    "        sensitivity = recall_score(y_test_np, predictions)\n",
    "        print(\"Sensitivity/Recall: \", sensitivity)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test_np, predictions).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        print(\"Specificity: \", specificity)\n",
    "        precision = precision_score(y_test_np, predictions)\n",
    "        print(\"Precision: \", precision)\n",
    "        f1 = f1_score(y_test_np, predictions)\n",
    "        print(\"F1 Score: \", f1)\n",
    "        y_prob = self.sigmoid(np.dot(X,self.theta))\n",
    "        auroc = roc_auc_score(y_test_np, y_prob)\n",
    "        print(\"AUROC: \", auroc)\n",
    "        precision_values, recall_values, _ = precision_recall_curve(y_test_np, y_prob)\n",
    "        aupr = auc(recall_values, precision_values)\n",
    "        print(\"AUPR: \", aupr)\n",
    "    \n",
    "    def train(self):\n",
    "        self.logisticRegression(X_train,y_train)\n",
    "        predictions = self.predict(X_test)\n",
    "        accuracy = np.mean(predictions == y_test.flatten())\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        self.output(X_test,predictions)\n",
    "\n",
    "def normalize(X):\n",
    "    return (X - X.mean()) / X.std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(model_list,n_base_learners,learning_rate,maxIteration,noFeatures):\n",
    "    global X_train,X_val,y_train,y_val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    # X_val = X_val.to_numpy()\n",
    "    # y_val = y_val.to_numpy()\n",
    "    meta_features = np.zeros((X_val.shape[0], n_base_learners))\n",
    "    for i in range(n_base_learners):\n",
    "        # Make predictions on the validation set using each base learner\n",
    "        meta_features[:, i] = model_list[i].predict(X_val)\n",
    "    # print(meta_features.shape)\n",
    "    # Step 3: Train a meta-model (another Logistic Regression) using the meta-features\n",
    "    print(X_val.shape)\n",
    "    print(meta_features.shape)\n",
    "    X=np.concatenate((X_val,meta_features),axis=1)\n",
    "    print(X.shape)\n",
    "    # Train the meta-model on meta-features and validation labels\n",
    "    LR_meta_final=LogisticRegression(maxIteration,X.shape[1],learning_rate)\n",
    "    LR_meta_final.logisticRegression(X,y_val)\n",
    "\n",
    "    # Step 5: Make final predictions on the test set using the base learners and the meta-model\n",
    "    # Generate meta-features from the test set\n",
    "    meta_features_test = np.zeros((X_test.shape[0], n_base_learners))\n",
    "\n",
    "    for i in range(n_base_learners):\n",
    "        # Use each base learner to predict on the test set\n",
    "        meta_features_test[:, i] = model_list[i].predict(X_test)\n",
    "    X=np.concatenate((X_test,meta_features_test),axis=1)\n",
    "    # Make final predictions using the meta-model\n",
    "    stacking_predictions = LR_meta_final.predict(X)\n",
    "\n",
    "    # Step 6: Evaluate the performance of the stacking ensemble\n",
    "    stacking_accuracy = np.mean(stacking_predictions == y_test.flatten())\n",
    "\n",
    "    print(f\"Stacking Ensemble Accuracy: {stacking_accuracy}\")\n",
    "    LR_meta_final.output(X,stacking_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting(model_list):\n",
    "    # Step 1: Collect predictions from each model in the model_list\n",
    "    predictions = np.zeros((len(model_list), X_test.shape[0]))\n",
    "\n",
    "    for i, model in enumerate(model_list):\n",
    "        predictions[i, :] = model.predict(X_test)\n",
    "\n",
    "    # Step 2: Perform majority voting\n",
    "    final_predictions = np.apply_along_axis(lambda x: np.bincount(x.astype(int)).argmax(), axis=0, arr=predictions)\n",
    "\n",
    "    # Step 3: Evaluate performance\n",
    "    y_test_flattened = y_test.flatten()\n",
    "    voting_accuracy = np.mean(final_predictions == y_test_flattened)\n",
    "    print(f\"Majority Voting Ensemble Accuracy: {voting_accuracy}\")\n",
    "    sensitivity = recall_score(y_test_flattened, final_predictions)\n",
    "    print(f\"Sensitivity/Recall: {sensitivity}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_flattened, final_predictions).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "\n",
    "    # Precision\n",
    "    precision = precision_score(y_test_flattened, final_predictions)\n",
    "    print(f\"Precision: {precision}\")\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_test_flattened, final_predictions)\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "    # AUROC\n",
    "    y_prob = np.mean([model.sigmoid(np.dot(X_test,model.theta)) for model in model_list], axis=0)  # Average the predicted probabilities\n",
    "    auroc = roc_auc_score(y_test_flattened, y_prob)\n",
    "    print(f\"AUROC: {auroc}\")\n",
    "\n",
    "    # AUPR\n",
    "    precision_values, recall_values, _ = precision_recall_curve(y_test_flattened, y_prob)\n",
    "    aupr = auc(recall_values, precision_values)\n",
    "    print(f\"AUPR: {aupr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(n_base_learners,maxIteration,learning_rate):\n",
    "    metrics_data = {\n",
    "    \"accuracy\": [],\n",
    "    \"sensitivity\": [],\n",
    "    \"specificity\": [],\n",
    "    \"precision\": [],\n",
    "    \"f1_score\": [],\n",
    "    \"auroc\": [],\n",
    "    \"aupr\": []\n",
    "    }\n",
    "    model_list=[]\n",
    "    noFeatures = X_train.shape[1]\n",
    "    for i in range(n_base_learners):\n",
    "        X_bootstrap, y_bootstrap = resample(X_train, y_train, random_state=i)\n",
    "        X = X_bootstrap\n",
    "        y = y_bootstrap.reshape(-1,1)\n",
    "        \n",
    "        LR=LogisticRegression(maxIteration,noFeatures,learning_rate)\n",
    "        LR.logisticRegression(X_bootstrap,y_bootstrap)\n",
    "        # Store the parameters (theta and bias) of the trained base learner\n",
    "        model_list.append(LR)\n",
    "\n",
    "        predictions = LR.predict(X_test)\n",
    "        accuracy = np.mean(predictions == y_test.flatten())\n",
    "        # LR.output()\n",
    "        # Predict on test data\n",
    "        model=model_list[-1]\n",
    "        predictions = model.predict(X_test)\n",
    "        y_test_flattened = y_test.flatten()\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = np.mean(predictions == y_test_flattened)\n",
    "        metrics_data['accuracy'].append(accuracy)\n",
    "\n",
    "        # Sensitivity/Recall\n",
    "        sensitivity = recall_score(y_test_flattened, predictions)\n",
    "        metrics_data['sensitivity'].append(sensitivity)\n",
    "\n",
    "        # Specificity\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test_flattened, predictions).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        metrics_data['specificity'].append(specificity)\n",
    "\n",
    "        # Precision\n",
    "        precision = precision_score(y_test_flattened, predictions)\n",
    "        metrics_data['precision'].append(precision)\n",
    "\n",
    "        # F1 Score\n",
    "        f1 = f1_score(y_test_flattened, predictions)\n",
    "        metrics_data['f1_score'].append(f1)\n",
    "\n",
    "        # AUROC\n",
    "        y_prob = model.sigmoid(np.dot(X_test,model.theta))  # Probabilities for the positive class\n",
    "        auroc = roc_auc_score(y_test_flattened, y_prob)\n",
    "        metrics_data['auroc'].append(auroc)\n",
    "\n",
    "        # AUPR\n",
    "        precision_values, recall_values, _ = precision_recall_curve(y_test_flattened, y_prob)\n",
    "        aupr = auc(recall_values, precision_values)\n",
    "        metrics_data['aupr'].append(aupr)\n",
    "    \n",
    "    average_accuracy = np.mean(metrics_data['accuracy'])\n",
    "    std_dev_accuracy = np.std(metrics_data['accuracy'])\n",
    "\n",
    "    average_sensitivity = np.mean(metrics_data['sensitivity'])\n",
    "    std_dev_sensitivity = np.std(metrics_data['sensitivity'])\n",
    "\n",
    "    average_specificity = np.mean(metrics_data['specificity'])\n",
    "    std_dev_specificity = np.std(metrics_data['specificity'])\n",
    "\n",
    "    average_precision = np.mean(metrics_data['precision'])\n",
    "    std_dev_precision = np.std(metrics_data['precision'])\n",
    "\n",
    "    average_f1 = np.mean(metrics_data['f1_score'])\n",
    "    std_dev_f1 = np.std(metrics_data['f1_score'])\n",
    "\n",
    "    average_auc_roc = np.mean(metrics_data['auroc'])\n",
    "    std_dev_auc_roc = np.std(metrics_data['auroc'])\n",
    "\n",
    "    average_aupr = np.mean(metrics_data['aupr'])\n",
    "    std_dev_aupr = np.std(metrics_data['aupr'])\n",
    "\n",
    "    print(f\"Bagging LR Learners - Accuracy: {average_accuracy:.4f} ± {std_dev_accuracy:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {average_sensitivity:.4f} ± {std_dev_sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {average_specificity:.4f} ± {std_dev_specificity:.4f}\")\n",
    "    print(f\"Precision: {average_precision:.4f} ± {std_dev_precision:.4f}\")\n",
    "    print(f\"F1 Score: {average_f1:.4f} ± {std_dev_f1:.4f}\")\n",
    "    print(f\"AUROC: {average_auc_roc:.4f} ± {std_dev_auc_roc:.4f}\")\n",
    "    print(f\"AUPR: {average_aupr:.4f} ± {std_dev_aupr:.4f}\")\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "    # Plot violin plots for each metric\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Create subplots for each performance metric\n",
    "    for i, metric in enumerate(metrics_data.keys()):\n",
    "        plt.subplot(2, 4, i+1)  # 2 rows, 4 columns of subplots\n",
    "        sns.violinplot(data=metrics_df[metric])\n",
    "        plt.title(f'{metric.capitalize()} Violin Plot')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    stacking(model_list,n_base_learners,learning_rate,maxIteration,noFeatures)\n",
    "    majority_voting(model_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalingFunction(scaling='standard'):\n",
    "    if scaling == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaling == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "def preprocessing(dataframe,target_col_name):\n",
    "    dataframe.replace(' ', np.nan, inplace=True)\n",
    "    dataframe.replace(' ?', np.nan, inplace=True)\n",
    "    # drop null and fill null\n",
    "    dataframe.dropna(subset=[target_col_name], inplace=True)\n",
    "    dataframe.fillna(dataframe.mean(numeric_only=True),inplace=True)\n",
    "    # fill null for non-numeric columns\n",
    "    non_numerical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "    for column in non_numerical_columns:\n",
    "        mode_value = dataframe[column].mode()[0]\n",
    "        dataframe[column].fillna(mode_value,inplace=True)\n",
    "    # drop duplicates\n",
    "    dataframe.drop_duplicates(inplace=True)\n",
    "    # feature and target\n",
    "    features=dataframe.drop(target_col_name,axis=1)\n",
    "    target=dataframe[target_col_name]\n",
    "    # print(features.shape)\n",
    "    # print(target.shape)\n",
    "    # label encoding the target\n",
    "    encoder=LabelEncoder()\n",
    "    target=encoder.fit_transform(target)\n",
    "    # categorization and one-hot encoding\n",
    "    categorical_columns=features.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        features[col]=features[col].astype('category')\n",
    "    features=pd.get_dummies(features,columns=categorical_columns)\n",
    "    # print(features.shape)\n",
    "    # print(target.shape)\n",
    "    # scaling\n",
    "    candidate_columns=features.select_dtypes(exclude=['bool']).columns\n",
    "    # scaler=scalingFunction('standard')\n",
    "    scaler=scalingFunction('minmax')\n",
    "    features_scaled=features.copy()\n",
    "    features_scaled[candidate_columns]=scaler.fit_transform(features[candidate_columns])\n",
    "    return features_scaled,target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(features, target):\n",
    "    global X_train, X_test, y_train, y_test, X_val, y_val\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train = X_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_test = y_test.to_numpy()\n",
    "    # X_val = X_val.to_numpy()\n",
    "    # y_val = y_val.to_numpy()\n",
    "\n",
    "# def split_2(features, target,features2,target2):\n",
    "#     global X_train, X_test, y_train, y_test, X_val, y_val\n",
    "\n",
    "#     X_test = features2\n",
    "#     y_test = target2\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     X_train = X_train.to_numpy()\n",
    "#     X_test = X_test.to_numpy()\n",
    "#     y_train = y_train.to_numpy()\n",
    "#     y_test = y_test.to_numpy()\n",
    "#     X_val = X_val.to_numpy()\n",
    "#     y_val = y_val.to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_2():\n",
    "    global dataframe\n",
    "    # global dataframe2\n",
    "    adult = fetch_ucirepo(id=2) \n",
    "    # data (as pandas dataframes) \n",
    "    X = adult.data.features \n",
    "    y = adult.data.targets\n",
    "    y[y.columns[0]] = y[y.columns[0]].str.rstrip('.')\n",
    "    dataframe=pd.concat([X, y], axis=1) \n",
    "\n",
    "    # X = adult.test.features \n",
    "    # y = adult.test.targets \n",
    "    # dataframe2=pd.concat([X, y], axis=1)\n",
    "\n",
    "    # column_file = 'adult/adult.names'\n",
    "    # data_file = 'adult/adult.data'\n",
    "    # test_file='adult/adult.test'\n",
    "    # columns = []\n",
    "    # with open(column_file, 'r') as f:\n",
    "    #     for line in f:\n",
    "    #         if '|' not in line:  # Ignore lines starting with '|'\n",
    "    #             if ':' in line:\n",
    "    #                 # Extract the column name before the ':' punctuation mark\n",
    "    #                 col_name = line.split(':')[0].strip()\n",
    "    #                 columns.append(col_name)\n",
    "    # columns.append('income-exceeds')\n",
    "    # dataframe = pd.read_csv(data_file, header=None)\n",
    "    # # print(dataframe)\n",
    "    # dataframe.columns = columns\n",
    "    # dataframe.replace(' ?', np.nan, inplace=True)\n",
    "    # data_types = dataframe.dtypes\n",
    "    # # print(dataframe)\n",
    "    # csv_data = []\n",
    "    # with open(test_file, 'r') as file:\n",
    "    #     reader = csv.reader(file)\n",
    "    #     for row in reader:\n",
    "    #         # Skip lines starting with '|'\n",
    "    #         if len(row) > 0 and not row[0].startswith('|'):\n",
    "    #             csv_data.append(row)\n",
    "    # dataframe2=pd.DataFrame(csv_data)\n",
    "    # for col in dataframe2.columns:\n",
    "    #     dtype = data_types[col]\n",
    "    #     if dtype == 'int64':\n",
    "    #         dataframe2[col] = pd.to_numeric(dataframe2[col], errors='coerce').astype('int64')\n",
    "    #     elif dtype == 'float64':\n",
    "    #         dataframe2[col] = pd.to_numeric(dataframe2[col], errors='coerce').astype('float64')\n",
    "    #     else:\n",
    "    #         dataframe2[col] = dataframe2[col].astype(dtype)\n",
    "    # dataframe2.columns = columns\n",
    "    # dataframe2.replace(' ?', np.nan, inplace=True)\n",
    "    # for col in dataframe.select_dtypes(include=['object']).columns:\n",
    "        # print(col)\n",
    "        # unique_values = dataframe[col].nunique()\n",
    "        # unique_values2 = dataframe2[col].nunique()\n",
    "        # print(unique_values)\n",
    "        # print(unique_values2)\n",
    "    # print(dataframe.head())\n",
    "\n",
    "def read(input):\n",
    "    global dataframe\n",
    "    # can I somehow assign the last column as target column\n",
    "    target_col_name=None\n",
    "    # file 1\n",
    "    if input == 1:\n",
    "        dataframe = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "        target_col_name='Churn'\n",
    "    # file 2\n",
    "    elif input == 2:\n",
    "        read_2()\n",
    "        target_col_name='income'\n",
    "    # file 3\n",
    "    elif input == 3:\n",
    "        dataframe = pd.read_csv('creditcard.csv')\n",
    "        target_col_name='Class'\n",
    "    else:\n",
    "        print(\"Invalid input\")\n",
    "    features,target=preprocessing(dataframe,target_col_name)\n",
    "    features_df=pd.DataFrame(features,columns=features.columns)\n",
    "    target_df=pd.DataFrame(target,columns=[target_col_name])\n",
    "    features_df_normalized=normalize(features_df)\n",
    "    split(features_df_normalized,target_df)\n",
    "    # if input==1 or input==3:\n",
    "    #     split(features_df_normalized,target_df)\n",
    "    # elif input==2:\n",
    "    #     features2,target2=preprocessing(dataframe2,target_col_name)\n",
    "    #     features_df2=pd.DataFrame(features2,columns=features2.columns)\n",
    "    #     target_df2=pd.DataFrame(target2,columns=[target_col_name])\n",
    "    #     features_df_normalized2=normalize(features_df2)\n",
    "    #     split_2(features_df_normalized,target_df,features_df_normalized2,target_df2)\n",
    "    #     # print(features_df_normalized.shape)\n",
    "    #     # print(target_df.shape)\n",
    "    #     # print(features_df_normalized2.shape)\n",
    "    #     # print(target_df2.shape)\n",
    "    maxIteration=1000\n",
    "    noFeatures=features_df_normalized.shape[1]\n",
    "    learningRate=0.01\n",
    "    n_base_learners=9\n",
    "    LR=LogisticRegression(maxIteration,noFeatures,learningRate)\n",
    "    # LR.train()\n",
    "    bagging(n_base_learners,maxIteration,learningRate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = int(input(\"Enter 1, 2 or 3: \"))\n",
    "read(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
