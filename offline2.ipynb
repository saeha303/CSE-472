{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve,accuracy_score\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=None\n",
    "\n",
    "def sigmoid(z):\n",
    "    z = np.array(z, dtype=float)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logisticRegression(X, y, theta, bias, learning_rate, maxIteration, noFeatures):\n",
    "    # print('hi')\n",
    "    m=len(X)\n",
    "    for iteration in range(maxIteration):\n",
    "        # matrix multiplication\n",
    "        h = sigmoid(np.dot(X,theta)+bias)\n",
    "        gradient = np.dot(X.T,(h - y))/m\n",
    "        db=np.sum(h-y)/m\n",
    "        theta -= learning_rate * gradient\n",
    "        bias -= learning_rate * db\n",
    "    return theta,bias\n",
    "\n",
    "def predict(X, theta,bias):\n",
    "    probabilities = sigmoid(np.dot(X,theta)+bias)\n",
    "    predictions = [1 if prob >= 0.5 else 0 for prob in probabilities]\n",
    "    return np.array(predictions)\n",
    "\n",
    "def normalize(X):\n",
    "    return (X - X.mean()) / X.std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalingFunction(scaling='standard'):\n",
    "    if scaling == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaling == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "def preprocessing(target_col_name):\n",
    "    # drop null and fill null\n",
    "    dataframe.dropna(subset=[target_col_name], inplace=True)\n",
    "    dataframe.fillna(dataframe.mean(numeric_only=True),inplace=True)\n",
    "    # fill null for non-numeric columns\n",
    "    non_numerical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "    for column in non_numerical_columns:\n",
    "        mode_value = dataframe[column].mode()[0]\n",
    "        dataframe[column].fillna(mode_value,inplace=True)\n",
    "    # drop duplicates\n",
    "    dataframe.drop_duplicates(inplace=True)\n",
    "    # feature and target\n",
    "    features=dataframe.drop(target_col_name,axis=1)\n",
    "    target=dataframe[target_col_name]\n",
    "    # label encoding the target\n",
    "    encoder=LabelEncoder()\n",
    "    target=encoder.fit_transform(target)\n",
    "    # categorization and one-hot encoding\n",
    "    categorical_columns=features.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        features[col]=features[col].astype('category')\n",
    "    features=pd.get_dummies(features,columns=categorical_columns)\n",
    "    # scaling\n",
    "    candidate_columns=features.select_dtypes(exclude=['bool']).columns\n",
    "    # scaler=scalingFunction('standard')\n",
    "    scaler=scalingFunction('minmax')\n",
    "    features_scaled=features.copy()\n",
    "    features_scaled[candidate_columns]=scaler.fit_transform(features[candidate_columns])\n",
    "    return features_scaled,target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(features,target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    return X_train,X_test,X_val,y_train,y_test,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(X_test,y_test,theta_final,bias_final):\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    y_test_np = y_test.to_numpy().flatten()\n",
    "\n",
    "    predictions = predict(X_test_np, theta_final,bias_final)\n",
    "    accuracy = np.mean(predictions == y_test_np)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    sensitivity = recall_score(y_test, predictions)\n",
    "    print(\"Sensitivity/Recall: \", sensitivity)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    print(\"Precision: \", precision)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    y_prob = sigmoid(np.dot(X_test_np,theta_final))\n",
    "    auroc = roc_auc_score(y_test, y_prob)\n",
    "    print(\"AUROC: \", auroc)\n",
    "    precision_values, recall_values, _ = precision_recall_curve(y_test, y_prob)\n",
    "    aupr = auc(recall_values, precision_values)\n",
    "    print(\"AUPR: \", aupr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features_scaled,target,target_col_name):\n",
    "    # transform to dataframe\n",
    "    features_df=pd.DataFrame(features_scaled,columns=features_scaled.columns)\n",
    "    target_df=pd.DataFrame(target,columns=[target_col_name])\n",
    "    features_df_normalized=normalize(features_df)\n",
    "\n",
    "    X_train, X_test, X_val,y_train, y_test, y_val = split(features_df_normalized, target_df)\n",
    "    # to numpy array\n",
    "    X = X_train.to_numpy()\n",
    "    y = y_train.to_numpy().reshape(-1,1)\n",
    "    maxIteration=1000\n",
    "    # noFeatures=features_df.shape[1]\n",
    "    noFeatures=features_df_normalized.shape[1]\n",
    "    # I was stuck here....\n",
    "    theta = np.zeros((noFeatures,1))\n",
    "    # print(theta.shape)\n",
    "    bias=0\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    theta_final,bias_final= logisticRegression(X, y, theta,bias, learning_rate, maxIteration, noFeatures)\n",
    "    output(X_test,y_test,theta_final,bias_final)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(X_val,X_test,y_val,y_test,theta_list,bias_list,n_base_learners,learning_rate,maxIteration):\n",
    "    meta_features = np.zeros((X_val.shape[0], n_base_learners))\n",
    "    for i in range(n_base_learners):\n",
    "        # Make predictions on the validation set using each base learner\n",
    "        meta_features[:, i] = predict(X_val.to_numpy(), theta_list[i], bias_list[i])\n",
    "    print(meta_features.shape)\n",
    "    # Step 3: Train a meta-model (another Logistic Regression) using the meta-features\n",
    "    theta_meta = np.zeros((n_base_learners, 1))  # Initialize theta for meta-model (number of base learners as features)\n",
    "    bias_meta = 0\n",
    "\n",
    "    # Train the meta-model on meta-features and validation labels\n",
    "    theta_meta_final, bias_meta_final = logisticRegression(meta_features, y_val.to_numpy(), theta_meta, bias_meta, learning_rate, maxIteration, n_base_learners)\n",
    "\n",
    "    # Step 5: Make final predictions on the test set using the base learners and the meta-model\n",
    "    # Generate meta-features from the test set\n",
    "    meta_features_test = np.zeros((X_test.shape[0], n_base_learners))\n",
    "\n",
    "    for i in range(n_base_learners):\n",
    "        # Use each base learner to predict on the test set\n",
    "        meta_features_test[:, i] = predict(X_test.to_numpy(), theta_list[i], bias_list[i])\n",
    "\n",
    "    # Make final predictions using the meta-model\n",
    "    stacking_predictions = predict(meta_features_test, theta_meta_final, bias_meta_final)\n",
    "\n",
    "    # Step 6: Evaluate the performance of the stacking ensemble\n",
    "    stacking_accuracy = np.mean(stacking_predictions == y_test.to_numpy())\n",
    "\n",
    "    print(f\"Stacking Ensemble Accuracy: {stacking_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(features_scaled,target,target_col_name):\n",
    "    features_df=pd.DataFrame(features_scaled,columns=features_scaled.columns)\n",
    "    target_df=pd.DataFrame(target,columns=[target_col_name])\n",
    "    features_df_normalized=normalize(features_df)\n",
    "\n",
    "    X_train, X_test, X_val,y_train, y_test, y_val = split(features_df_normalized, target_df)\n",
    "    theta_list = []\n",
    "    bias_list = []\n",
    "    n_base_learners = 9\n",
    "    maxIteration = 1000 \n",
    "    learning_rate = 0.01\n",
    "    noFeatures = X_train.shape[1]\n",
    "    for i in range(n_base_learners):\n",
    "        X_bootstrap, y_bootstrap = resample(X_train, y_train, random_state=i)\n",
    "        X = X_bootstrap.to_numpy()\n",
    "        y = y_bootstrap.to_numpy().reshape(-1,1)\n",
    "        # Initialize theta and bias for the current model\n",
    "        theta = np.zeros((noFeatures, 1))\n",
    "        bias = 0\n",
    "\n",
    "        # Train Logistic Regression on the bootstrap sample\n",
    "        theta_final, bias_final = logisticRegression(X, y, theta, bias, learning_rate, maxIteration, noFeatures)\n",
    "        \n",
    "        # Store the parameters (theta and bias) of the trained base learner\n",
    "        theta_list.append(theta_final)\n",
    "        bias_list.append(bias_final)\n",
    "        output(X_test,y_test,theta_final,bias_final)\n",
    "    stacking(X_val,X_test,y_val,y_test,theta_list,bias_list,n_base_learners,learning_rate,maxIteration)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(max_iter=maxIteration)\n",
    "    # model.fit(X_train, y_train)\n",
    "    # y_pred = model.predict(X_test_np)\n",
    "    # print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_2():\n",
    "    global dataframe\n",
    "    column_file = 'adult/adult.names'\n",
    "    data_file = 'adult/adult.data'\n",
    "    columns = []\n",
    "    with open(column_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if '|' not in line:  # Ignore lines starting with '|'\n",
    "                if ':' in line:\n",
    "                    # Extract the column name before the ':' punctuation mark\n",
    "                    col_name = line.split(':')[0].strip()\n",
    "                    columns.append(col_name)\n",
    "    columns.append('income-exceeds')\n",
    "    dataframe = pd.read_csv(data_file, header=None)\n",
    "    # print(dataframe)\n",
    "    dataframe.columns = columns\n",
    "    # print(dataframe)\n",
    "    dataframe.replace(' ?', np.nan, inplace=True)\n",
    "\n",
    "def read(input):\n",
    "    global dataframe\n",
    "    target_col_name=None\n",
    "    # file 1\n",
    "    if input == 1:\n",
    "        dataframe = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "        target_col_name='Churn'\n",
    "    # file 2\n",
    "    elif input == 2:\n",
    "        read_2()\n",
    "        target_col_name='income-exceeds'\n",
    "    # file 3\n",
    "    elif input == 3:\n",
    "        dataframe = pd.read_csv('creditcard.csv')\n",
    "        target_col_name='Class'\n",
    "    else:\n",
    "        print(\"Invalid input\")\n",
    "    feature,target=preprocessing(target_col_name)\n",
    "    # train(feature,target,target_col_name)\n",
    "    bagging(feature,target,target_col_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13932\\2834924140.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataframe[column].fillna(mode_value,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8409649661954518\n",
      "Sensitivity/Recall:  0.6412975670617592\n",
      "Specificity:  0.9062181447502549\n",
      "Precision:  0.6908602150537635\n",
      "F1 Score:  0.6651569071497897\n",
      "AUROC:  0.8948738444672102\n",
      "AUPR:  0.7419782770899328\n",
      "Accuracy: 0.8401966810079902\n",
      "Sensitivity/Recall:  0.6188396756082346\n",
      "Specificity:  0.9125382262996942\n",
      "Precision:  0.6980999296270233\n",
      "F1 Score:  0.656084656084656\n",
      "AUROC:  0.8937780397737933\n",
      "AUPR:  0.7386554270721246\n",
      "Accuracy: 0.8380454824830977\n",
      "Sensitivity/Recall:  0.6138490330630069\n",
      "Specificity:  0.9113149847094801\n",
      "Precision:  0.693446088794926\n",
      "F1 Score:  0.6512243547319656\n",
      "AUROC:  0.8949412512090289\n",
      "AUPR:  0.7414784590188622\n",
      "Accuracy: 0.8394283958205285\n",
      "Sensitivity/Recall:  0.620711166562695\n",
      "Specificity:  0.910907237512742\n",
      "Precision:  0.6948324022346368\n",
      "F1 Score:  0.6556836902800659\n",
      "AUROC:  0.8947783303858782\n",
      "AUPR:  0.7419707488490423\n",
      "Accuracy: 0.8388137676705593\n",
      "Sensitivity/Recall:  0.6350592638802246\n",
      "Specificity:  0.9054026503567788\n",
      "Precision:  0.6869095816464238\n",
      "F1 Score:  0.659967585089141\n",
      "AUROC:  0.8946374121407172\n",
      "AUPR:  0.7407281530046349\n",
      "Accuracy: 0.8400430239704979\n",
      "Sensitivity/Recall:  0.6294447910168434\n",
      "Specificity:  0.908868501529052\n",
      "Precision:  0.6929945054945055\n",
      "F1 Score:  0.6596927100359594\n",
      "AUROC:  0.8949501539862501\n",
      "AUPR:  0.7410837245455608\n",
      "Accuracy: 0.8397357098955132\n",
      "Sensitivity/Recall:  0.6512788521522146\n",
      "Specificity:  0.9013251783893985\n",
      "Precision:  0.6832460732984293\n",
      "F1 Score:  0.6668795911849249\n",
      "AUROC:  0.894827041295532\n",
      "AUPR:  0.7407523808449574\n",
      "Accuracy: 0.8378918254456054\n",
      "Sensitivity/Recall:  0.6381784154709919\n",
      "Specificity:  0.9031600407747197\n",
      "Precision:  0.6829105473965287\n",
      "F1 Score:  0.6597871654305063\n",
      "AUROC:  0.8940538986851234\n",
      "AUPR:  0.7406833075416673\n",
      "Accuracy: 0.8383527965580824\n",
      "Sensitivity/Recall:  0.627573300062383\n",
      "Specificity:  0.9072375127420998\n",
      "Precision:  0.6885694729637235\n",
      "F1 Score:  0.6566579634464752\n",
      "AUROC:  0.8942638770450156\n",
      "AUPR:  0.7404597772250215\n",
      "(5206, 9)\n",
      "Stacking Ensemble Accuracy: 0.6442293707635707\n"
     ]
    }
   ],
   "source": [
    "user_input = int(input(\"Enter 1, 2 or 3: \"))\n",
    "read(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
