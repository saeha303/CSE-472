{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler,MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, precision_recall_curve,accuracy_score\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=None\n",
    "X_train = None\n",
    "X_test = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "X_val = None\n",
    "y_val = None\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self,maxIteration,noFeatures,learning_rate):\n",
    "        self.maxIteration=maxIteration\n",
    "        self.noFeatures=noFeatures\n",
    "        self.theta = np.zeros((noFeatures,1))\n",
    "        self.bias=0\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        z = np.array(z, dtype=float)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def logisticRegression(self,X,y):\n",
    "        m=len(X)\n",
    "        for iteration in range(self.maxIteration):\n",
    "            # matrix multiplication\n",
    "            h = self.sigmoid(np.dot(X,self.theta)+self.bias)\n",
    "            gradient = np.dot(X.T,(h - y))/m\n",
    "            db=np.sum(h-y)/m\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self,X):\n",
    "        probabilities = self.sigmoid(np.dot(X,self.theta)+self.bias)\n",
    "        predictions = [1 if prob >= 0.5 else 0 for prob in probabilities]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def output(self,X,predictions):\n",
    "        y_test_np = y_test.flatten()\n",
    "\n",
    "        # predictions = self.predict(X)\n",
    "        # accuracy = np.mean(predictions == y_test_np)\n",
    "        # print(f\"Accuracy: {accuracy}\")\n",
    "        sensitivity = recall_score(y_test_np, predictions)\n",
    "        print(\"Sensitivity/Recall: \", sensitivity)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test_np, predictions).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        print(\"Specificity: \", specificity)\n",
    "        precision = precision_score(y_test_np, predictions)\n",
    "        print(\"Precision: \", precision)\n",
    "        f1 = f1_score(y_test_np, predictions)\n",
    "        print(\"F1 Score: \", f1)\n",
    "        y_prob = self.sigmoid(np.dot(X,self.theta))\n",
    "        auroc = roc_auc_score(y_test_np, y_prob)\n",
    "        print(\"AUROC: \", auroc)\n",
    "        precision_values, recall_values, _ = precision_recall_curve(y_test_np, y_prob)\n",
    "        aupr = auc(recall_values, precision_values)\n",
    "        print(\"AUPR: \", aupr)\n",
    "    \n",
    "    def train(self):\n",
    "        self.logisticRegression(X_train,y_train)\n",
    "        predictions = self.predict(X_test)\n",
    "        accuracy = np.mean(predictions == y_test.flatten())\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        self.output(X_test,predictions)\n",
    "\n",
    "def normalize(X):\n",
    "    return (X - X.mean()) / X.std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(model_list,n_base_learners,learning_rate,maxIteration,noFeatures):\n",
    "    global X_train,X_val,y_train,y_val\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    # X_val = X_val.to_numpy()\n",
    "    # y_val = y_val.to_numpy()\n",
    "    meta_features = np.zeros((X_val.shape[0], n_base_learners))\n",
    "    for i in range(n_base_learners):\n",
    "        meta_features[:, i] = model_list[i].predict(X_val)\n",
    "    # print(X_val.shape)\n",
    "    # print(meta_features.shape)\n",
    "    X=np.concatenate((X_val,meta_features),axis=1)\n",
    "    # print(X.shape)\n",
    "\n",
    "    LR_meta_final=LogisticRegression(maxIteration,X.shape[1],learning_rate)\n",
    "    LR_meta_final.logisticRegression(X,y_val)\n",
    "    meta_features_test = np.zeros((X_test.shape[0], n_base_learners))\n",
    "    for i in range(n_base_learners):\n",
    "        meta_features_test[:, i] = model_list[i].predict(X_test)\n",
    "\n",
    "    X=np.concatenate((X_test,meta_features_test),axis=1)\n",
    "    stacking_predictions = LR_meta_final.predict(X)\n",
    "    stacking_accuracy = np.mean(stacking_predictions == y_test.flatten())\n",
    "\n",
    "    print(f\"Stacking Ensemble Accuracy: {stacking_accuracy}\")\n",
    "    LR_meta_final.output(X,stacking_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting(model_list):\n",
    "    predictions = np.zeros((len(model_list), X_test.shape[0]))\n",
    "\n",
    "    for i, model in enumerate(model_list):\n",
    "        predictions[i, :] = model.predict(X_test)\n",
    "\n",
    "    final_predictions = np.apply_along_axis(lambda x: np.bincount(x.astype(int)).argmax(), axis=0, arr=predictions)\n",
    "\n",
    "    y_test_flattened = y_test.flatten()\n",
    "    voting_accuracy = np.mean(final_predictions == y_test_flattened)\n",
    "    print(f\"Majority Voting Ensemble Accuracy: {voting_accuracy}\")\n",
    "    sensitivity = recall_score(y_test_flattened, final_predictions)\n",
    "    print(f\"Sensitivity/Recall: {sensitivity}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_flattened, final_predictions).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    precision = precision_score(y_test_flattened, final_predictions)\n",
    "    print(f\"Precision: {precision}\")\n",
    "    f1 = f1_score(y_test_flattened, final_predictions)\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    y_prob = np.mean([model.sigmoid(np.dot(X_test,model.theta)) for model in model_list], axis=0)  # Average the predicted probabilities\n",
    "    auroc = roc_auc_score(y_test_flattened, y_prob)\n",
    "    print(f\"AUROC: {auroc}\")\n",
    "    precision_values, recall_values, _ = precision_recall_curve(y_test_flattened, y_prob)\n",
    "    aupr = auc(recall_values, precision_values)\n",
    "    print(f\"AUPR: {aupr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagging(n_base_learners,maxIteration,learning_rate):\n",
    "    metrics_data = {\n",
    "    \"accuracy\": [],\n",
    "    \"sensitivity\": [],\n",
    "    \"specificity\": [],\n",
    "    \"precision\": [],\n",
    "    \"f1_score\": [],\n",
    "    \"auroc\": [],\n",
    "    \"aupr\": []\n",
    "    }\n",
    "    model_list=[]\n",
    "    noFeatures = X_train.shape[1]\n",
    "    for i in range(n_base_learners):\n",
    "        X_bootstrap, y_bootstrap = resample(X_train, y_train, random_state=i)\n",
    "        \n",
    "        LR=LogisticRegression(maxIteration,noFeatures,learning_rate)\n",
    "        LR.logisticRegression(X_bootstrap,y_bootstrap)\n",
    "        model_list.append(LR)\n",
    "\n",
    "        predictions = LR.predict(X_test)\n",
    "        accuracy = np.mean(predictions == y_test.flatten())\n",
    "        # LR.output()\n",
    "        model=model_list[-1]\n",
    "        predictions = model.predict(X_test)\n",
    "        y_test_flattened = y_test.flatten()\n",
    "        accuracy = np.mean(predictions == y_test_flattened)\n",
    "        metrics_data['accuracy'].append(accuracy)\n",
    "        sensitivity = recall_score(y_test_flattened, predictions)\n",
    "        metrics_data['sensitivity'].append(sensitivity)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test_flattened, predictions).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        metrics_data['specificity'].append(specificity)\n",
    "        precision = precision_score(y_test_flattened, predictions)\n",
    "        metrics_data['precision'].append(precision)\n",
    "        f1 = f1_score(y_test_flattened, predictions)\n",
    "        metrics_data['f1_score'].append(f1)\n",
    "        y_prob = model.sigmoid(np.dot(X_test,model.theta))\n",
    "        auroc = roc_auc_score(y_test_flattened, y_prob)\n",
    "        metrics_data['auroc'].append(auroc)\n",
    "        precision_values, recall_values, _ = precision_recall_curve(y_test_flattened, y_prob)\n",
    "        aupr = auc(recall_values, precision_values)\n",
    "        metrics_data['aupr'].append(aupr)\n",
    "    \n",
    "    average_accuracy = np.mean(metrics_data['accuracy'])\n",
    "    std_dev_accuracy = np.std(metrics_data['accuracy'])\n",
    "    average_sensitivity = np.mean(metrics_data['sensitivity'])\n",
    "    std_dev_sensitivity = np.std(metrics_data['sensitivity'])\n",
    "    average_specificity = np.mean(metrics_data['specificity'])\n",
    "    std_dev_specificity = np.std(metrics_data['specificity'])\n",
    "    average_precision = np.mean(metrics_data['precision'])\n",
    "    std_dev_precision = np.std(metrics_data['precision'])\n",
    "    average_f1 = np.mean(metrics_data['f1_score'])\n",
    "    std_dev_f1 = np.std(metrics_data['f1_score'])\n",
    "    average_auc_roc = np.mean(metrics_data['auroc'])\n",
    "    std_dev_auc_roc = np.std(metrics_data['auroc'])\n",
    "    average_aupr = np.mean(metrics_data['aupr'])\n",
    "    std_dev_aupr = np.std(metrics_data['aupr'])\n",
    "\n",
    "    print(f\"Bagging LR Learners - Accuracy: {average_accuracy:.4f} ± {std_dev_accuracy:.4f}\")\n",
    "    print(f\"Sensitivity (Recall): {average_sensitivity:.4f} ± {std_dev_sensitivity:.4f}\")\n",
    "    print(f\"Specificity: {average_specificity:.4f} ± {std_dev_specificity:.4f}\")\n",
    "    print(f\"Precision: {average_precision:.4f} ± {std_dev_precision:.4f}\")\n",
    "    print(f\"F1 Score: {average_f1:.4f} ± {std_dev_f1:.4f}\")\n",
    "    print(f\"AUROC: {average_auc_roc:.4f} ± {std_dev_auc_roc:.4f}\")\n",
    "    print(f\"AUPR: {average_aupr:.4f} ± {std_dev_aupr:.4f}\")\n",
    "\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for i, metric in enumerate(metrics_data.keys()):\n",
    "        plt.subplot(2, 4, i+1)  # 2 rows, 4 columns of subplots\n",
    "        sns.violinplot(data=metrics_df[metric])\n",
    "        plt.title(f'{metric.capitalize()} Violin Plot')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    stacking(model_list,n_base_learners,learning_rate,maxIteration,noFeatures)\n",
    "    majority_voting(model_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalingFunction(scaling='standard'):\n",
    "    if scaling == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif scaling == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "def preprocessing(dataframe,target_col_name):\n",
    "    # dataframe.replace(' ', np.nan, inplace=True)\n",
    "    # dataframe.replace('?', np.nan, inplace=True)\n",
    "    print(dataframe.isnull().sum())\n",
    "    # drop null and fill null\n",
    "    dataframe.dropna(subset=[target_col_name], inplace=True)\n",
    "    dataframe.fillna(dataframe.mean(numeric_only=True),inplace=True)\n",
    "    # fill null for non-numeric columns\n",
    "    non_numerical_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "    for column in non_numerical_columns:\n",
    "        mode_value = dataframe[column].mode()[0]\n",
    "        dataframe[column].fillna(mode_value,inplace=True)\n",
    "    print(dataframe.isnull().sum())\n",
    "    # drop duplicates\n",
    "    print(dataframe.duplicated().sum())\n",
    "    dataframe.drop_duplicates(inplace=True)\n",
    "    print(dataframe.duplicated().sum())\n",
    "    # feature and target\n",
    "    features=dataframe.drop(target_col_name,axis=1)\n",
    "    target=dataframe[target_col_name]\n",
    "    # label encoding the target\n",
    "    encoder=LabelEncoder()\n",
    "    target=encoder.fit_transform(target)\n",
    "    # categorization and one-hot encoding\n",
    "    categorical_columns=features.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        features[col]=features[col].astype('category')\n",
    "    features=pd.get_dummies(features,columns=categorical_columns)\n",
    "    print(features.dtypes)\n",
    "    # print(target.shape)\n",
    "    # scaling\n",
    "    candidate_columns=features.select_dtypes(exclude=['bool']).columns\n",
    "    scaler=scalingFunction('standard')\n",
    "    # scaler=scalingFunction('minmax')\n",
    "    features_scaled=features.copy()\n",
    "    features_scaled[candidate_columns]=scaler.fit_transform(features[candidate_columns])\n",
    "    return features_scaled,target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain/Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informationGain(n):\n",
    "    global X_train,X_test\n",
    "    mutual_info = mutual_info_classif(X_train, y_train)\n",
    "    feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Information Gain': mutual_info\n",
    "    })\n",
    "    feature_importance = feature_importance.sort_values(by='Information Gain', ascending=False)\n",
    "    # print(feature_importance)\n",
    "    top_n_features = feature_importance['Feature'].head(n)\n",
    "    X_train = X_train[top_n_features]\n",
    "    X_test = X_test[top_n_features]\n",
    "\n",
    "def correlationAnalysis(n,features,target):\n",
    "    global X_train,X_test\n",
    "    # features.reset_index(drop=True, inplace=True)\n",
    "    # target.reset_index(drop=True, inplace=True)\n",
    "    correlation=features.corrwith(target)\n",
    "    print(correlation)\n",
    "    top_n_features = correlation.abs().nlargest(n)\n",
    "    # print(top_n_features)\n",
    "    X_train=X_train[top_n_features]\n",
    "    X_test=X_test[top_n_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(features, target):\n",
    "    global X_train, X_test, y_train, y_test, X_val, y_val\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # X_train = X_train.to_numpy()\n",
    "    # X_test = X_test.to_numpy()\n",
    "    # y_train = y_train.to_numpy()\n",
    "    # y_test = y_test.to_numpy()\n",
    "    # X_val = X_val.to_numpy()\n",
    "    # y_val = y_val.to_numpy()\n",
    "def convertToNumpy():\n",
    "    global X_train, X_test, y_train, y_test, X_val, y_val\n",
    "    X_train = X_train.to_numpy()\n",
    "    X_test = X_test.to_numpy()\n",
    "    y_train = y_train.to_numpy()\n",
    "    y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_2():\n",
    "    global dataframe\n",
    "    adult = fetch_ucirepo(id=2) \n",
    "    # data (as pandas dataframes) \n",
    "    X = adult.data.features \n",
    "    y = adult.data.targets\n",
    "    y[y.columns[0]] = y[y.columns[0]].str.rstrip('.')\n",
    "    dataframe=pd.concat([X, y], axis=1) \n",
    "\n",
    "\n",
    "def read(input):\n",
    "    global dataframe\n",
    "    # can I somehow assign the last column as target column\n",
    "    target_col_name=None\n",
    "    # file 1\n",
    "    if input == 1:\n",
    "        dataframe = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "        target_col_name='Churn'\n",
    "        dataframe['TotalCharges'] = dataframe['TotalCharges'].str.strip()\n",
    "        dataframe['TotalCharges'].replace('', np.nan, inplace=True)\n",
    "        dataframe['TotalCharges'] = pd.to_numeric(dataframe['TotalCharges'], errors='coerce')\n",
    "    # file 2\n",
    "    elif input == 2:\n",
    "        read_2()\n",
    "        target_col_name='income'\n",
    "        dataframe.replace('?', np.nan, inplace=True)\n",
    "    # file 3\n",
    "    elif input == 3:\n",
    "        dataframe = pd.read_csv('creditcard.csv')\n",
    "        target_col_name='Class'\n",
    "    else:\n",
    "        print(\"Invalid input\")\n",
    "    features,target=preprocessing(dataframe,target_col_name)\n",
    "    # print(dataframe.dtypes)\n",
    "    features_df=pd.DataFrame(features,columns=features.columns)\n",
    "    target_df=pd.DataFrame(target,columns=[target_col_name])\n",
    "    target_series=targ\n",
    "    features_df_normalized=normalize(features_df)\n",
    "    features_df_normalized.reset_index(drop=True, inplace=True)\n",
    "    target_df.reset_index(drop=True, inplace=True)\n",
    "    split(features_df_normalized,target_df)\n",
    "    print(features_df_normalized.shape)\n",
    "    print(target_df.dtypes)\n",
    "    # print(target_df)\n",
    "    n_feature_selection=100\n",
    "    # information gain is done on training data\n",
    "    informationGain(n_feature_selection)\n",
    "    # correlation analysis\n",
    "    correlationAnalysis(n_feature_selection,features_df_normalized,target_df)\n",
    "    # print(features_df_normalized.shape)\n",
    "    convertToNumpy()\n",
    "    maxIteration=1000\n",
    "    noFeatures=features_df.shape[1]\n",
    "    learningRate=0.01\n",
    "    n_base_learners=9\n",
    "    LR=LogisticRegression(maxIteration,noFeatures,learningRate)\n",
    "    # LR.train()\n",
    "    bagging(n_base_learners,maxIteration,learningRate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_12408\\3119145519.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y[y.columns[0]] = y[y.columns[0]].str.rstrip('.')\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_12408\\977013730.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataframe[column].fillna(mode_value,inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                  0\n",
      "workclass         2799\n",
      "fnlwgt               0\n",
      "education            0\n",
      "education-num        0\n",
      "marital-status       0\n",
      "occupation        2809\n",
      "relationship         0\n",
      "race                 0\n",
      "sex                  0\n",
      "capital-gain         0\n",
      "capital-loss         0\n",
      "hours-per-week       0\n",
      "native-country     857\n",
      "income               0\n",
      "dtype: int64\n",
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education         0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital-gain      0\n",
      "capital-loss      0\n",
      "hours-per-week    0\n",
      "native-country    0\n",
      "income            0\n",
      "dtype: int64\n",
      "53\n",
      "0\n",
      "age                               int64\n",
      "fnlwgt                            int64\n",
      "education-num                     int64\n",
      "capital-gain                      int64\n",
      "capital-loss                      int64\n",
      "                                  ...  \n",
      "native-country_Thailand            bool\n",
      "native-country_Trinadad&Tobago     bool\n",
      "native-country_United-States       bool\n",
      "native-country_Vietnam             bool\n",
      "native-country_Yugoslavia          bool\n",
      "Length: 105, dtype: object\n",
      "(48789, 105)\n",
      "income    int32\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "user_input = int(input(\"Enter 1, 2 or 3: \"))\n",
    "read(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
