{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix,f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Xavier initialization\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * np.sqrt(2 / (input_dim+output_dim))\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # print(f'{self.__class__.__name__}')\n",
    "        # print(X.shape)\n",
    "        self.input = X\n",
    "        # Wx+b\n",
    "        self.output = np.dot(X, self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dL_doutput):\n",
    "        # Gradients\n",
    "        dL_dinput = np.dot(dL_doutput, self.weights.T)\n",
    "        dL_dweights = np.dot(self.input.T, dL_doutput)\n",
    "        dL_dbias = np.sum(dL_doutput, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.grads = {\n",
    "            'weights': dL_dweights,\n",
    "            'bias': dL_dbias\n",
    "        }\n",
    "        return dL_dinput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, input_dim, epsilon=1e-5, momentum=0.9):\n",
    "        self.input_dim=input_dim\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        # scale parameter\n",
    "        self.gamma = np.ones((1, self.input_dim))\n",
    "        # shift parameter\n",
    "        self.beta = np.zeros((1, self.input_dim))\n",
    "        self.moving_mean = np.zeros((1, self.input_dim))\n",
    "        self.moving_var = np.ones((1, self.input_dim))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        # print(f'{self.__class__.__name__}')\n",
    "        # print(X.shape)\n",
    "        if training:\n",
    "            # page 296, something about len(X.shape)\n",
    "            self.mean = np.mean(X, axis=0, keepdims=True)\n",
    "            self.var = np.var(X, axis=0, keepdims=True)\n",
    "            self.X_centered = X - self.mean\n",
    "            self.std = np.sqrt(self.var + self.epsilon)\n",
    "            self.X_norm=self.X_centered/self.std\n",
    "\n",
    "            # Update running statistics\n",
    "            self.moving_mean = self.momentum * self.moving_mean + (1 - self.momentum) * self.mean\n",
    "            self.moving_var = self.momentum * self.moving_var + (1 - self.momentum) * self.var\n",
    "        else:\n",
    "            self.X_norm = (X - self.moving_mean) / np.sqrt(self.moving_var + self.epsilon)\n",
    "        \n",
    "        return self.gamma * self.X_norm + self.beta\n",
    "    # confused about the backward propagation\n",
    "    def backward(self, dL_doutput):\n",
    "        m = dL_doutput.shape[0]\n",
    "        dL_dgamma = np.sum(dL_doutput * self.X_norm, axis=0, keepdims=True)\n",
    "        dL_dbeta = np.sum(dL_doutput, axis=0, keepdims=True)\n",
    "        dL_dX_norm = dL_doutput * self.gamma\n",
    "        dL_dvar = np.sum(dL_dX_norm * self.X_centered * -0.5 / (self.std ** 3), axis=0, keepdims=True)\n",
    "        dL_dmean = np.sum(dL_dX_norm * -1 / self.std, axis=0, keepdims=True) + dL_dvar * np.mean(-2 * self.X_centered, axis=0, keepdims=True)\n",
    "        \n",
    "        dL_dinput = dL_dX_norm / self.std + dL_dvar * 2 * self.X_centered / m + dL_dmean / m\n",
    "        \n",
    "        self.grads = {\n",
    "            'gamma': dL_dgamma,\n",
    "            'beta': dL_dbeta\n",
    "        }\n",
    "        \n",
    "        return dL_dinput\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        # print(f'{self.__class__.__name__}')\n",
    "        # print(X.shape)\n",
    "        self.output = np.maximum(0, X)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dL_doutput):\n",
    "        return dL_doutput * (self.output > 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply after activation function, page 177\n",
    "class Dropout:\n",
    "\n",
    "    def __init__(self, dropout):\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        # print(f'{self.__class__.__name__}')\n",
    "        # print(X.shape)\n",
    "        if training:\n",
    "            assert 0 <= self.dropout <= 1\n",
    "            # In this case, all elements are dropped out\n",
    "            if self.dropout == 1:\n",
    "                return np.zeros_like(X)\n",
    "            # In this case, all elements are kept\n",
    "            if self.dropout == 0:\n",
    "                return X\n",
    "            self.mask = np.random.uniform(0, 1, X.shape) > self.dropout\n",
    "            # self.mask = np.random.binomial(1, 1 - self.dropout, size=X.shape)\n",
    "\n",
    "            return X * self.mask.astype(np.float32) / (1.0 - self.dropout)\n",
    "\n",
    "        return X\n",
    "\n",
    "    # confused about the back propagation\n",
    "    def backward(self, dL_doutput):\n",
    "        return dL_doutput * self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.005, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        # momentum\n",
    "        self.v = {}\n",
    "        # second momentum\n",
    "        self.s = {}\n",
    "        # time step counter\n",
    "        self.t = 0\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        self.t += 1\n",
    "        for param_name in params:\n",
    "            if param_name not in self.v.keys():\n",
    "                self.v[param_name] = np.zeros_like(grads[param_name])\n",
    "                self.s[param_name] = np.zeros_like(grads[param_name])\n",
    "            \n",
    "            # Momentum estimates\n",
    "            self.v[param_name] = self.beta1 * self.v[param_name] + (1 - self.beta1) * grads[param_name]\n",
    "            self.s[param_name] = self.beta2 * self.s[param_name] + (1 - self.beta2) * np.square(grads[param_name])\n",
    "            \n",
    "            v_bias_corr = self.v[param_name] / (1 - self.beta1 ** self.t)\n",
    "            s_bias_corr = self.s[param_name] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            params[param_name] -= self.learning_rate * v_bias_corr / (np.sqrt(s_bias_corr) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, X):\n",
    "        # print(f'{self.__class__.__name__}')\n",
    "        # print(X.shape)\n",
    "        exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dL_doutput):\n",
    "        # In multi-class classification, the gradient of cross-entropy with softmax simplifies the backward pass.\n",
    "        return dL_doutput\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed-Forward Neural Network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "\n",
    "        # print('start')\n",
    "        # print(f'{self.__class__.__name__}')\n",
    "        # print(X.shape)\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                X = layer.forward(X, training)\n",
    "            else:\n",
    "                X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, dL_doutput):\n",
    "        for layer in reversed(self.layers):\n",
    "            dL_doutput = layer.backward(dL_doutput)\n",
    "\n",
    "    def get_params_and_grads(self):\n",
    "        params_and_grads = {}\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Layer) or isinstance(layer, BatchNormalization):\n",
    "                if isinstance(layer, Layer):\n",
    "                    params_and_grads.update({\n",
    "                        f'{layer.__class__.__name__}_weights': layer.weights,\n",
    "                        f'{layer.__class__.__name__}_bias': layer.bias,\n",
    "                        f'{layer.__class__.__name__}_grads_weights': layer.grads['weights'],\n",
    "                        f'{layer.__class__.__name__}_grads_bias': layer.grads['bias']\n",
    "                    })\n",
    "                if isinstance(layer, BatchNormalization):\n",
    "                    params_and_grads.update({\n",
    "                        f'{layer.__class__.__name__}_gamma': layer.gamma,\n",
    "                        f'{layer.__class__.__name__}_beta': layer.beta,\n",
    "                        f'{layer.__class__.__name__}_grads_gamma': layer.grads['gamma'],\n",
    "                        f'{layer.__class__.__name__}_grads_beta': layer.grads['beta']\n",
    "                    })\n",
    "        return params_and_grads\n",
    "\n",
    "    def train(self, X_train, y_train, epochs, batch_size,filepath):\n",
    "        optimizer = AdamOptimizer(learning_rate=0.005)\n",
    "        for epoch in range(epochs):\n",
    "            # Divide into batches\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                X_batch = X_train[i:i+batch_size]\n",
    "                y_batch = y_train[i:i+batch_size]\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = self.forward(X_batch, training=True)\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(predictions + 1e-8), axis=1))  # Cross-entropy loss\n",
    "                \n",
    "                # Backward pass\n",
    "                dL_doutput = predictions - y_batch  # Gradient of cross-entropy\n",
    "                self.backward(dL_doutput)\n",
    "                \n",
    "                # Get params and gradients, update using Adam optimizer\n",
    "                params_and_grads = self.get_params_and_grads()\n",
    "                optimizer.update({k: params_and_grads[k] for k in params_and_grads if 'grads' not in k},\n",
    "                                 {k.split('grads_')[0]+k.split('grads_')[1]: params_and_grads[k] for k in params_and_grads if 'grads' in k})\n",
    "                if epoch==epochs-1 and i+batch_size>=len(X_train):\n",
    "                    self.save_model(filepath)\n",
    "                \n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        return self.forward(X_test, training=False)\n",
    "    \n",
    "    def evaluate(self,X_test,y_test):\n",
    "        prediction = self.predict(X_test)\n",
    "        y_pred=[[1 if prob == max(row) else 0 for prob in row] for row in prediction]\n",
    "        y_pred=np.array(y_pred)\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "        loss = -np.mean(np.sum(y_test * np.log(prediction + 1e-8), axis=1))  # Cross-entropy loss\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"Loss: {loss}\")\n",
    "        # y_test=[[j if k == 1 else 0 for j,k in enumerate(i)] for i in y_test]\n",
    "        # y_test=[max(i) for i in y_test]\n",
    "        # y_pred=[[j if k == 1 else 0 for j,k in enumerate(i)] for i in y_pred]\n",
    "        # y_pred=[max(i) for i in y_pred]\n",
    "        tn, fp, fn, tp = 0,0,0,0\n",
    "        for i,row in enumerate(y_test):\n",
    "            for j,element in enumerate(row):\n",
    "                # print(element,y_pred[j])\n",
    "                if element==1 and y_pred[i][j]==1:\n",
    "                    tp+=1\n",
    "                elif element==1 and y_pred[i][j]==0:\n",
    "                    fn+=1\n",
    "        for i,row in enumerate(y_pred):\n",
    "            for j,element in enumerate(row):\n",
    "                # print(element,y_pred[j])\n",
    "                if element==0:\n",
    "                    continue\n",
    "                elif element==1 and y_test[i][j]==0:\n",
    "                    fp+=1\n",
    "        tn=len(y_test)-fp\n",
    "        print('tn\\tfp\\tfn\\ttp')\n",
    "        print(f'{tn}\\t{fp}\\t{fn}\\t{tp}')\n",
    "        # print(y_test[:10])\n",
    "        # print(y_pred[:10])\n",
    "        specificity = tn / (tn + fp)\n",
    "        precision = tp/(tp+fp)\n",
    "        f1 = 2*specificity*precision/(specificity+precision)\n",
    "        # print(f1)\n",
    "        print(\"F1 Score: \", f1)\n",
    "        return accuracy,loss,f1\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        params = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Layer):\n",
    "                params[f\"layer_{i}_weights\"] = layer.weights\n",
    "                params[f\"layer_{i}_bias\"] = layer.bias\n",
    "            if isinstance(layer, BatchNormalization):\n",
    "                params[f\"layer_{i}_gamma\"] = layer.gamma\n",
    "                params[f\"layer_{i}_beta\"] = layer.beta\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Layer):\n",
    "                layer.weights = params[f\"layer_{i}_weights\"]\n",
    "                layer.bias = params[f\"layer_{i}_bias\"]\n",
    "            if isinstance(layer, BatchNormalization):\n",
    "                layer.gamma = params[f\"layer_{i}_gamma\"]\n",
    "                layer.beta = params[f\"layer_{i}_beta\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.ToTensor()\n",
    "train_dataset=datasets.FashionMNIST(root='./data',train=True,transform=transform,download=True)\n",
    "test_dataset=datasets.FashionMNIST(root='./data',train=False,transform=transform,download=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "\n",
    "X_train, y_train = next(iter(train_loader))\n",
    "X_train = X_train.reshape(-1, 28*28).numpy()  # Flatten the images\n",
    "y_train=np.eye(10)[y_train]\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture\n",
    "network = NeuralNetwork([\n",
    "    Layer(input_dim=28*28, output_dim=128),\n",
    "    BatchNormalization(input_dim=128),\n",
    "    ReLU(),\n",
    "    Dropout(dropout=0.2),\n",
    "    Layer(input_dim=128, output_dim=128),\n",
    "    BatchNormalization(input_dim=128),\n",
    "    ReLU(),\n",
    "    Dropout(0.5),\n",
    "    Layer(input_dim=128, output_dim=10),\n",
    "    Softmax()\n",
    "])\n",
    "\n",
    "# here Layer(784, 128) means input layer has 784 neurons and hidden layer has 128 neurons\n",
    "# Layer(128, 10) that hidden layer with 128 layers plus output layer with 10 neurons\n",
    "\n",
    "folder_path = './pickle/'\n",
    "filename = 'weights.pkl'\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "filepath = os.path.join(folder_path, filename)\n",
    "network.train(X_train,y_train,100,64,filepath)\n",
    "\n",
    "# network.save_model(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val shape: (12000, 784)\n",
      "y_val shape: (12000, 10)\n",
      "Accuracy: 94.81%\n",
      "Loss: 0.7423473425440631\n",
      "tn\tfp\tfn\ttp\n",
      "9\t1\t1\t9\n",
      "F1 Score:  0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.94815, 0.7423473425440631, 0.9)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val, y_val = next(iter(val_loader))\n",
    "X_val = X_val.reshape(-1, 28*28).numpy()  # Flatten the images\n",
    "y_val=np.eye(10)[y_val]\n",
    "# print(y_val[:10])\n",
    "\n",
    "# y_val=[[j if k == 1 else 0 for j,k in enumerate(i)] for i in y_val]\n",
    "# y_val=[max(i) for i in y_val]\n",
    "# print(y_val[:10])\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "network.evaluate(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperparameter we'll change is the number of hidden layers, ranging from 1 to 5\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "layers=[]\n",
    "layers.append(Layer(input_dim=28*28, output_dim=256))\n",
    "for i, layer_size in enumerate(range(1,6,1)):\n",
    "    print ('{}: Testing neural network MLP classifier with {} hidden layers with constant number of layers'.format(i+1, layer_size))\n",
    "    if layer_size!=1:\n",
    "        layers.pop()\n",
    "        layers.pop()\n",
    "        layers.append(Layer(256,256))\n",
    "    layers.append(BatchNormalization(256))\n",
    "    layers.append(ReLU())\n",
    "    layers.append(Dropout(0.2))\n",
    "    layers.append(Layer(256,10))\n",
    "    layers.append(Softmax())\n",
    "    model = NeuralNetwork(layers)\n",
    "\n",
    "    # Train\n",
    "    model.train(X_train, y_train,10,64,filepath)\n",
    "\n",
    "    current_accuracy,current_loss,current_f1 = model.evaluate(X_val, y_val)\n",
    "\n",
    "    if best_accuracy < current_accuracy:\n",
    "        best_model=model\n",
    "        best_accuracy = current_accuracy\n",
    "\n",
    "    del (model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperparameter we'll change is the number of neurons and hidden layers, ranging from 1 to 5 for hidden layers and n_neurons for neurons\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "n_neurons=[784,512,256,128,64,32,16,10]\n",
    "layers=[]\n",
    "layers.append(Layer(input_dim=n_neurons[0], output_dim=n_neurons[1]))\n",
    "for i, layer_size in enumerate(range(1,len(n_neurons)-1,1)):\n",
    "    print ('{}: Testing neural network MLP classifier with {} hidden layers and variable number of neurons at each layer'.format(i+1, layer_size))\n",
    "    if layer_size!=1:\n",
    "        layers.pop()\n",
    "        layers.pop()\n",
    "        layers.append(Layer(n_neurons[layer_size-1],n_neurons[layer_size]))\n",
    "    layers.append(BatchNormalization(n_neurons[layer_size]))\n",
    "    layers.append(ReLU())\n",
    "    layers.append(Dropout(0.2))\n",
    "    layers.append(Layer(n_neurons[layer_size],n_neurons[-1]))\n",
    "    layers.append(Softmax())\n",
    "    model = NeuralNetwork(layers)\n",
    "\n",
    "    # Train\n",
    "    model.train(X_train, y_train,10,64,filepath)\n",
    "\n",
    "    current_accuracy,current_loss,current_f1 = model.evaluate(X_val, y_val)\n",
    "\n",
    "    if best_accuracy < current_accuracy:\n",
    "        best_model=model\n",
    "        best_accuracy = current_accuracy\n",
    "\n",
    "    del (model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperparameter we'll change is the number of neurons in the hidden layer, ranging from n_neurons[0] to n_neurons[-1]\n",
    "accuracies = []\n",
    "models = []\n",
    "n_neurons=[784,512,256,128,64,32,16,10]\n",
    "\n",
    "for i, layer_size in enumerate(range(1,len(n_neurons)-1,1)):\n",
    "    print ('{}: Testing neural network MLP classifier with {} neurons in hidden layer'.format(i+1, n_neurons[layer_size]))\n",
    "    layers=[]\n",
    "    layers.append(Layer(input_dim=n_neurons[0], output_dim=n_neurons[layer_size]))\n",
    "    layers.append(BatchNormalization(n_neurons[layer_size]))\n",
    "    layers.append(ReLU())\n",
    "    layers.append(Dropout(0.2))\n",
    "    layers.append(Layer(n_neurons[layer_size],n_neurons[-1]))\n",
    "    layers.append(Softmax())\n",
    "    model = NeuralNetwork(layers)\n",
    "\n",
    "    # Train\n",
    "    model.train(X_train, y_train,10,64,filepath)\n",
    "\n",
    "    current_accuracy,current_loss,current_f1 = model.evaluate(X_val, y_val)\n",
    "    \n",
    "    models.append(model)\n",
    "    accuracies.append(current_accuracy)\n",
    "\n",
    "    del (model)\n",
    "sorted_models = pd.DataFrame({\n",
    "    'Models': models,\n",
    "    'Accuracies': accuracies\n",
    "    })\n",
    "sorted_models = sorted_models.sort_values(by='Accuracies', ascending=False)\n",
    "sorted_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_model(filepath)\n",
    "test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "X_test, y_test = next(iter(test_loader))\n",
    "X_test = X_test.reshape(-1, 28*28).numpy()\n",
    "y_test=np.eye(10)[y_test]\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "network.evaluate(X_test,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
